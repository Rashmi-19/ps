import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_curve, average_precision_score, ndcg_score

# -----------------------------
# Example data
# -----------------------------
# Ground truth relevance (1 = relevant, 0 = not relevant)
y_true = np.array([1, 0, 1, 1, 0, 0, 1, 0, 1, 0])

# Predicted relevance scores (from retrieval system)
y_scores = np.array([0.95, 0.6, 0.9, 0.85, 0.4, 0.3, 0.8, 0.2, 0.7, 0.1])

# Sort by predicted score (ranked retrieval)
sorted_idx = np.argsort(y_scores)[::-1]
y_true_sorted = y_true[sorted_idx]
y_scores_sorted = y_scores[sorted_idx]

# -----------------------------
# Unranked / Set-based Retrieval
# -----------------------------
y_pred_binary = (y_scores >= 0.5).astype(int)

precision = precision_score(y_true, y_pred_binary)
recall = recall_score(y_true, y_pred_binary)
f1 = f1_score(y_true, y_pred_binary)

print("Unranked Retrieval Metrics")
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1, "\n")

# -----------------------------
# Ranked Retrieval
# -----------------------------
def precision_at_k(y_true, k):
    return np.mean(y_true[:k])

def r_precision(y_true):
    R = np.sum(y_true)  # total relevant
    return np.mean(y_true[:R])

def mean_reciprocal_rank(y_true):
    for i, val in enumerate(y_true, start=1):
        if val == 1:
            return 1.0 / i
    return 0.0

def average_precision(y_true):
    precisions = []
    rel_count = 0
    for i, val in enumerate(y_true, start=1):
        if val == 1:
            rel_count += 1
            precisions.append(rel_count / i)
    return np.mean(precisions) if precisions else 0.0

# Precision@k
print("Ranked Retrieval Metrics")
print("Precision@3:", precision_at_k(y_true_sorted, 3))
print("R-Precision:", r_precision(y_true_sorted))
print("MRR:", mean_reciprocal_rank(y_true_sorted))
print("MAP:", average_precision(y_true_sorted))

# -----------------------------
# PR Graph (built-in sklearn)
# -----------------------------
precisions, recalls, thresholds = precision_recall_curve(y_true, y_scores)
ap = average_precision_score(y_true, y_scores)

plt.figure(figsize=(6, 5))
plt.plot(recalls, precisions, marker='.')
plt.title(f'Precision-Recall Curve (AP={ap:.2f})')
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.grid(True)
plt.show()

# -----------------------------
# Graded Relevance (nDCG)
# -----------------------------
# Assume graded relevance (0 = not relevant, 3 = highly relevant)
y_true_graded = np.array([[3, 0, 2, 2, 0, 0, 1, 0, 2, 0]])
y_scores_graded = np.array([[0.95, 0.6, 0.9, 0.85, 0.4, 0.3, 0.8, 0.2, 0.7, 0.1]])

ndcg = ndcg_score(y_true_graded, y_scores_graded, k=5)
print("\nGraded Relevance Metric")
print("nDCG@5:", ndcg)

# Plot nDCG Curve
ks = [1, 3, 5, 10]
ndcgs = [ndcg_score(y_true_graded, y_scores_graded, k=k) for k in ks]

plt.figure(figsize=(6, 5))
plt.plot(ks, ndcgs, marker='o')
plt.title("nDCG Curve")
plt.xlabel("k")
plt.ylabel("nDCG")
plt.grid(True)
plt.show()
